{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Million Russian Troll Tweets - Part I\n",
    "- James M Irving, Ph.D.\n",
    "- Mod 4 Project\n",
    "- Flatiron Full Time Data Science Bootcamp - 02/2019 Cohort\n",
    "\n",
    "## GOAL: Using Twitter API to Extract Control Tweets\n",
    "\n",
    "- *IF I can get a control dataset* of non-Troll tweets from same time period with similar hashtags:*\n",
    "    - Use NLP to predict of a tweet is from an authentic user or a Russian troll.\n",
    "- *If no control tweets to compare to*\n",
    "    - Use NLP to predict how many retweets a Troll tweet will get.\n",
    "    - Consider both raw # of retweets, as well as a normalized # of retweets/# of followers.\n",
    "        - The latter would give better indication of language's effect on propagation. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSPECTING TROLL TWEETS FROM KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bs_ds as bs\n",
    "from bs_ds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = 'russian-troll-tweets/'\n",
    "# os.listdir('russian-troll-tweets/')\n",
    "filelist = [os.path.join(root_dir,file) for file in os.listdir(root_dir) if file.endswith('.csv')]\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vertically concatenate \n",
    "df = pd.DataFrame()\n",
    "for file in filelist:\n",
    "    df_new = pd.read_csv(file)\n",
    "    df = pd.concat([df,df_new], axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Features:\n",
    "- Kaggle Dataset published by FiveThirtyEight\n",
    "    - https://www.kaggle.com/fivethirtyeight/russian-troll-tweets/downloads/russian-troll-tweets.zip/2\n",
    "<br>    \n",
    "- Data is split into 9 .csv files\n",
    "    - 'IRAhandle_tweets_1.csv' to 9\n",
    "\n",
    "- **Variables:**\n",
    "    - ~~`external_author_id` | An author account ID from Twitter~~\n",
    "    - `author` | The handle sending the tweet\n",
    "    - `content` | The text of the tweet\n",
    "    - `region` | A region classification, as [determined by Social Studio](https://help.salesforce.com/articleView?   id=000199367&type=1)\n",
    "    - `language` | The language of the tweet\n",
    "    - `publish_date` | The date and time the tweet was sent\n",
    "    - ~~`harvested_date` | The date and time the tweet was collected by Social Studio~~\n",
    "    - `following` | The number of accounts the handle was following at the time of the tweet\n",
    "    - `followers` | The number of followers the handle had at the time of the tweet\n",
    "    - `updates` | The number of “update actions” on the account that authored the tweet, including tweets, retweets and likes\n",
    "    - `post_type` | Indicates if the tweet was a retweet or a quote-tweet *[Whats a quote-tweet?]*\n",
    "    - `account_type` | Specific account theme, as coded by Linvill and Warren\n",
    "    - `retweet` | A binary indicator of whether or not the tweet is a retweet [?]\n",
    "    - `account_category` | General account theme, as coded by Linvill and Warren\n",
    "    - `new_june_2018` | A binary indicator of whether the handle was newly listed in June 2018\n",
    "    \n",
    "### **Classification of account_type**\n",
    "Taken from: [rcmediafreedom.eu summary](https://www.rcmediafreedom.eu/Publications/Academic-sources/Troll-Factories-The-Internet-Research-Agency-and-State-Sponsored-Agenda-Building)\n",
    "\n",
    ">- **They identified five categories of IRA-associated Twitter accounts, each with unique patterns of behaviors:**\n",
    "    - **Right Troll**, spreading nativist and right-leaning populist messages. It supported the candidacy and Presidency of Donald Trump and denigrated the Democratic Party. It often sent divisive messages about mainstream and moderate Republicans.\n",
    "    - **Left Troll**, sending socially liberal messages and discussing gender, sexual, religious, and -especially- racial identity. Many tweets seemed intentionally divisive, attacking mainstream Democratic politicians, particularly Hillary Clinton, while supporting Bernie Sanders prior to the election.\n",
    "    - **News Feed**, overwhelmingly presenting themselves as U.S. local news aggregators, linking to legitimate regional news sources and tweeting about issues of local interest.\n",
    "    - **Hashtag Gamer**, dedicated almost exclusively to playing hashtag games.\n",
    "    - **Fearmonger**: spreading a hoax about poisoned turkeys near the 2015 Thanksgiving holiday.\n",
    "\n",
    ">The different types of account were used differently and their efforts were conducted systematically, with different allocation when faced with different political circumstances or shifting goals. E.g.: there was a spike of activity by right and left troll accounts before the publication of John Podesta's emails by WikiLeaks. According to the authors, this activity can be characterised as “industrialized political warfare”.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from Inspection / Pandas_Profiling ProfileReport\n",
    "\n",
    "- **Language to Analyze is in `Content`:**\n",
    "    - Actual tweet contents. \n",
    " \n",
    "- **Classification/Analysis Thoughts:**\n",
    "    - **Variables should be considered in 2 ways:**\n",
    "        - First, the tweet contents. \n",
    "            - Use NLP to engineer features to feed into deep learning.\n",
    "                - Sentiment analysis, named-entity frequency/types, most-similar words. \n",
    "        - Second, the tweet metadata. \n",
    "        \n",
    "### Thoughts on specific features:\n",
    "- `language`\n",
    "    - There are 56 unique languages. \n",
    "    - 2.4 million are English, 670 K are in Russian, etc.\n",
    "\n",
    "### Questions to answer:\n",
    "- [x] Why are so many post_types missing? (55%?)\n",
    "    - Because they were added 'new_june_2018' and were not classified by the original scientists. \n",
    "- [x] How many tweets were written by a russian troll account?\n",
    "    - After removing retweets, there are 1,272,848 original tweets. \n",
    "    \n",
    "### Scrubing to Perform\n",
    "- **Recast Columns:**\n",
    "    - [ ] `publish_date` to datetime. \n",
    "- **Columns to Discard:**\n",
    "    - [ ] `harvested_date` (we care about publish_date, if anything, time-wise)\n",
    "    - [ ] `language`: remove all non-english tweets and drop column\n",
    "    - [ ] `new_june_2018`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Targeted Tweets to Language=English and Retweet=0 Only\n",
    "\n",
    "- Since the goal is to use NLP to detect which tweets came from Russian trolls, we will only analyze the tweets that were originally created by a known Russian troll account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-english rows\n",
    "df = df.loc[df.language=='English']\n",
    "df = df.loc[df.retweet==0]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop harvested_date and new_june_2018\n",
    "cols_to_drop = ['harvested_date','new_june_2018']#: remove all non-english tweets and drop column\n",
    "\n",
    "for col in cols_to_drop:\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Save/Load and Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_or_load = input('Would you like to \"save\" or \"load\" dataframe?\\n(\"save\",\"load\",\"no\"):')\n",
    "\n",
    "# if save_or_load.lower()=='save':\n",
    "#     # Save csv\n",
    "#     df.to_csv('russian_troll_tweets_eng_only_date_pub_index.csv')\n",
    "    \n",
    "# if save_or_load.lower()=='load':\n",
    "#     import bs_ds as bs\n",
    "#     from bs_ds.imports import *\n",
    "#     # Load csva\n",
    "#     df = pd.read_csv('russian_troll_tweets_eng_only_date_pub_index.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i,'\\t',np.random.choice(df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recasting Publish date as datetime column (date_published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recast date_published as datetime and make index\n",
    "df['date_published'] = pd.to_datetime(df['publish_date'])\n",
    "df.set_index('date_published', inplace=True)\n",
    "print('Changed index to datetime \"date_published\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publish_date to datetime\n",
    "# df['date_published'] = pd.to_datetime(df.publish_date)\n",
    "print(f'Tweet dates from {np.min(df.index)}  to  {np.max(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TwitterAPI to Harvest Control Tweets\n",
    "\n",
    "## My Search Strategy\n",
    "\n",
    "- **We need non-troll Tweets to use as a control for the Troll tweets. Ideally, these would be from the same time period covered by the Troll tweets.**\n",
    "\n",
    "    - However, extracting batch historical tweets from the same time period is not an option. This would **require Twitter Enterprise level** Developer Membership (which costs **\\\\$2,000 per month**)\n",
    "    - The free Twitter developer account access allows extracting Tweets from the last 7 days. We will have to work within this limitation for harvesting control tweets.\n",
    "    - Due to the temporal difference, extra care must be put into Tweet search strategy.\n",
    "\n",
    "\n",
    "\n",
    "**Inspect Data to get search parameters:**\n",
    "- [X] Get the date range for the English tweets in the original dataset<br>\n",
    "    - **Tweet date range:**\n",
    "        - **2012-02-06** to **2018-05-30**\n",
    "\n",
    "- [X] Get a list of the hash tags (and their frequencies from the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Data to get search parameters:\n",
    "print(f'Tweet date range:\\n {min(df.index)} to {max(df.index)}')\n",
    "print(f'\\nTotal days: {max(df.index)-min(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Hashtags & @'s to search for\n",
    "\n",
    "- Use regular expressions to extract the hashtags #words and @handles.\n",
    "- Use the top X many tags as search terms for twitter API\n",
    "    - There are _1,678,170 unique hashtags_ and _1,165,744 unique @'s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Make a column containing all hashtags and mentions\n",
    "import re\n",
    "hashtags = re.compile(r'(\\#\\w*)')\n",
    "# df['hashtags'] = df['content'].map(lambda x: hashtags.findall(str(x)))\n",
    "\n",
    "mentions = re.compile(r'(\\@\\w*)')\n",
    "# df['mentions'] = df['content'].map(lambda x: mentions.findall(str(x)))\n",
    "\n",
    "urls = re.compile(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\")\n",
    "# df['links'] = df['content'].map(lambda x: urls.findall(str(x)))\n",
    "\n",
    "# Testing individual re's from above\n",
    "hashtag_list = []\n",
    "hashtag_list = df['content'].map(lambda x: hashtags.findall(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "all_hashtags = []\n",
    "for i in range(len(hashtag_list)):\n",
    "    if len(hashtag_list[i])==0:\n",
    "        continue\n",
    "    elif len(hashtag_list[i])>1:\n",
    "        [all_hashtags.append(x) for x in hashtag_list[i]]\n",
    "\n",
    "    else:\n",
    "        all_hashtags.append(hashtag_list[i])\n",
    "    \n",
    "hashtag_counts = pd.Series(all_hashtags)\n",
    "hashtag_counts.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def get_tags_ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_tags_ats to accept a list of text entries and return all found tags and ats as 2 series/lists\n",
    "def get_tags_ats(text_to_search,exp_tag = r'(#\\w*)',exp_at = r'(@\\w*)', output='series',show_counts=False):\n",
    "    \"\"\"Accepts a list of text entries to search, and a regex for tags, and a regex for @'s.\n",
    "    Joins all entries in the list of text and then re.findsall() for both expressions.\n",
    "    Returns a series of found_tags and a series of found_ats.'\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Create a single long joined-list of strings\n",
    "    text_to_search_combined = ' '.join(text_to_search)\n",
    "        \n",
    "    # print(len(text_to_search_combined), len(text_to_search_list))\n",
    "    found_tags = re.findall(exp_tag, text_to_search_combined)\n",
    "    found_ats = re.findall(exp_at, text_to_search_combined)\n",
    "    \n",
    "    if output.lower() == 'series':\n",
    "        found_tags = pd.Series(found_tags, name='tags')\n",
    "        found_ats = pd.Series(found_ats, name='ats')\n",
    "        \n",
    "        if show_counts==True:\n",
    "            print(f'\\t{found_tags.name}:\\n{tweet_tags.value_counts()} \\n\\n\\t{found_ats.name}:\\n{tweet_ats.value_counts()}')\n",
    "                \n",
    "    if (output.lower() != 'series') & (show_counts==True):\n",
    "        raise Exception('output must be set to \"series\" in order to show_counts')\n",
    "                       \n",
    "    return found_tags, found_ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to get a list of hash tags.\n",
    "text_to_search_list = []\n",
    "\n",
    "for i in range(len(df)):    \n",
    "    tweet_contents =df['content'].iloc[i]\n",
    "    text_to_search_list.append(tweet_contents)\n",
    "\n",
    "text_to_search_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get all tweet tags and @'s from text_to_search_list\n",
    "tweet_tags, tweet_ats = get_tags_ats(text_to_search_list, show_counts=False)\n",
    "\n",
    "print(f\"There were {len(tweet_tags)} unique hashtags and {len(tweet_ats)} unique @'s\\n\")\n",
    "\n",
    "# Create a dataframe with top_tags\n",
    "df_top_tags = pd.DataFrame(tweet_tags.value_counts()[:40])#,'\\n')\n",
    "df_top_tags['% Total'] = (df_top_tags['tags']/len(tweet_tags)*100)\n",
    "\n",
    "# Create a dataframe with top_ats\n",
    "df_top_ats = pd.DataFrame(tweet_ats.value_counts()[:40])\n",
    "df_top_ats['% Total'] = (df_top_ats['ats']/len(tweet_ats)*100)\n",
    "\n",
    "# Display top tags and ats\n",
    "# bs.display_side_by_side(df_top_tags,df_top_ats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Top Tags and Ats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose list of top tags to use in search\n",
    "list_top_30_tags = df_top_tags.index[:30]\n",
    "list_top_30_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose list of top tags to use in search\n",
    "list_top_30_ats = df_top_ats.index[:30]\n",
    "list_top_30_ats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Tweet Search Strategy \n",
    "- **The most common hashtags include some very generic categories** that will not be appropriate control tweets.\n",
    "    - Examples:\n",
    "        - '#news','#sports','#politics','#world','#local','#TopNews','#health','#business','#tech'\n",
    "    - If we used these to extract control Tweets, our model would be biased, since many of these categories contain time-specific topics and would therefore be easy to predict vs. the troll tweets.\n",
    "  \n",
    "- **The most common @'s are much more revealing and helpful in narrowing the focus of the results.**\n",
    "    - Final decision is to use the top 40 mentions from the trolls tweets and extracting present-day Tweets with the same mentions. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Twitter Search API to Extract Control Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Required API key are saved in the Main folder in which this repo is saved. \n",
    "- [x] Check the [Premium account docs for search syntax](https://developer.twitter.com/en/docs/tweets/search/guides/premium-operators.html)\n",
    "- [x] [Check this article for using Tweepy for most efficient twitter api extraction](https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively./)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LINK TO PREMIUM SEARCH API GUIDE**<br>\n",
    "https://developer.twitter.com/en/docs/tweets/search/api-reference/premium-search\n",
    "\n",
    "**Available search operators**\n",
    "- Premium search API supports rules with up to 1,024 characters. The Search Tweets APIs support the premium operators listed below. See our Premium operators guide for more details.\n",
    "\n",
    "- The base URI for the premium search API is https://api.twitter.com/1.1/tweets/search/.\n",
    "\n",
    "**Matching on Tweet contents:**\n",
    "- keyword , \"quoted phrase\" , # , @, url , lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using `tweepy` to access twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def connect_twitter_api, def search_twitter_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Initialzie Tweepy with Authorization Keys    \n",
    "def connect_twitter_api(api_key, api_secret_key):\n",
    "    import tweepy, sys\n",
    "    auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    if (not api):\n",
    "        print(\"Can't authenticate.\")\n",
    "        sys.exit(-1)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def search_twitter_api(api_object, searchQuery, maxTweets, fName, tweetsPerQry=100, max_id=0, sinceId=None):\n",
    "    \"\"\"Take an authenticated tweepy api_object, a search queary, max# of tweets to retreive, a desintation filename.\n",
    "    Uses tweept.api.search for the searchQuery until maxTweets is reached, saved harvest tweets to fName.\"\"\"\n",
    "    import sys, jsonpickle, os\n",
    "    api = api_object\n",
    "    tweetCount = 0\n",
    "    print(f'Downloading max{maxTweets} for {searchQuery}...')\n",
    "    with open(fName, 'a+') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "\n",
    "            try:\n",
    "                if (max_id <=0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, since_id=sinceId, tweet_mode='extended')\n",
    "\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id-1), tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id-1),since_id=sinceId, tweet_mode='extended')\n",
    "\n",
    "                if not new_tweets:\n",
    "                    print('No more tweets found')\n",
    "                    break\n",
    "\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False)+'\\n')\n",
    "\n",
    "                tweetCount+=len(new_tweets)\n",
    "\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "    print (\"Downloaded {0} tweets, Saved to {1}\\n\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Twitter and Harvest Tweets\n",
    "\n",
    "### Making lists of tags and ats to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the # of each @ and each # that i want ot query, then make a query_dict to feed into the cell below\n",
    "query_ats = tuple(zip(df_top_ats.index, df_top_ats['ats']))\n",
    "query_tags = tuple(zip(df_top_tags.index, df_top_tags['tags']))\n",
    "\n",
    "# Calculate how many tweets are represented by the top 30 tags and top 30 @'s \n",
    "sum_top_tweet_tags = df_top_tags['tags'].sum()\n",
    "sum_top_tweet_ats = df_top_ats['ats'].sum()\n",
    "print(f\"Sum of top tags = {sum_top_tweet_tags}\\nSum of top @'s = {sum_top_tweet_ats}\")\n",
    "\n",
    "print(query_ats[:10],'\\n')\n",
    "print(query_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([x[1] for x in query_ats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Data to get search parameters:\n",
    "print(f'Tweet date range:\\n {min(df.index)} to {max(df.index)}')\n",
    "print(f'\\nTotal days: {max(df.index)-min(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to twitter api and searching for lists of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import API keys from text files (so not displayed here and not in repo)\n",
    "with open('../consumer_API_key.txt','r') as f:\n",
    "    api_key =  f.read()\n",
    "with open('../consumer_API_secret_key.txt','r') as f:\n",
    "    api_secret_key  = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually connecting to API and doing test searches. \n",
    "import tweepy, sys\n",
    "auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print(\"Can't authenticate.\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a batch of test results\n",
    "searchQuery='#politics'\n",
    "tweetsPerQry=100\n",
    "\n",
    "new_tweets = api.search(q=searchQuery, count=tweetsPerQry, tweet_mode='extended')\n",
    "type(new_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Display time range of new_tweets so i can define a timetrange to test\n",
    "test_dates = [x.created_at for x in new_tweets]\n",
    "print(f'Range:{min(test_dates)} to {max(test_dates)}')\n",
    "test_dates[0], test_dates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "end_time = datetime(2019,6,2,20,0,0)\n",
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINING A NEW FUNCTION TO EXAMINE THE NEW_TWEETS OUTPUTS\n",
    "def check_tweet_daterange(new_tweets,timerange_begin,timerange_end,verbose=0):\n",
    "    \"\"\"Examines specific information for each tweet in a tweepy searchResults object.\"\"\"\n",
    "    \n",
    "    time_start = timerange_begin\n",
    "    time_end = timerange_end\n",
    "    \n",
    "    # Pull out each tweet's status object. \n",
    "    idx_keep_tweets = []\n",
    "    for i,tweet in enumerate(new_tweets):\n",
    "        if (tweet.created_at > time_start) and (tweet.created_at < time_end):\n",
    "            idx_keep_tweets.append(i)\n",
    "            if verbose>0:\n",
    "                print(f'tweet({i} kept:{tweet.created_at})')\n",
    "    return idx_keep_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining search criteria to limit twitter results to\n",
    "latest_date = max(df.index) # Get latest date from troll tweets\n",
    "earliest_date = min(df.index) # Get the earliest date from troll tweets\n",
    "\n",
    "# Convert pandas timestamps to datetime object for tweet results\n",
    "latest_datetime = latest_date.to_pydatetime()\n",
    "earliest_datetime = earliest_date.to_pydatetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automated Searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = connect_twitter_api(api_key,api_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract tweets for top @'s, while matching the distribution of top @'s\n",
    "final_query_list = query_ats\n",
    "filename = 'tweets_for_top40_ats.txt'\n",
    "\n",
    "for q in final_query_list:\n",
    "    searchQuery = q[0]\n",
    "    maxTweets = q[1]\n",
    "    print(f'Query={searchQuery}, max={maxTweets}')\n",
    "    search_twitter_api(api, searchQuery, maxTweets, fName=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Extracted Tweets from API to match Troll Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats = pd.read_json('tweets_for_top40_ats.txt', lines=True)\n",
    "\n",
    "df_ats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats['entities'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ats.loc[df_ats['full_text'].str.contains('RT')]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re_RT=re.compile('RT \\@\\w*\\:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rt_tweets = df['full_text'][:10]\n",
    "example_rt_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rt_tweets.apply(lambda x: re_RT.sub('',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_list = (['idx','quoted_status'])\n",
    "for k,v in df['quoted_status'].items()[:10]:\n",
    "    print\n",
    "    quote_list.append([k,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweeted_status'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Making New Extracted Tweets Match Russian Troll Tweet Database\n",
    "\n",
    "- Columns to be renamed/reformatted to match troll tweets:\n",
    "    - `created_at` -> `date_published`-> index\n",
    "    - `full_text` -> `content`\n",
    "    - `df['user']`\n",
    "        - `.['followers_count']` -> `'following'`\n",
    "        - `.['followers_count']` -> `'followers'`\n",
    "\n",
    "- Columns missing from original troll tweets (to be removed).\n",
    "    -coordinates, favorited, favorite_count, display_text_range, withheld_in_countries    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats['lang'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats.user[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats.entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ats['user'][1]['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ats['user'][0]['id'])\n",
    "print(df_ats['user'][0]['screen_name'])\n",
    "print(df_ats['user'][0]['followers_count'])\n",
    "print(df_ats['user'][0]['following'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_row = df_ats.index[2]\n",
    "curr_row = df_ats.loc[df_ats.index==idx_row]\n",
    "curr_author = curr_row['user']\n",
    "curr_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_list =['external_author_id', 'author', 'content', 'region', 'following', 'followers', 'updates', 'post_type',\n",
    " 'account_type', 'retweet', 'account_category']\n",
    "df_export = pd.DataFrame(columns=df_columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.loc[0,'external_author_id']='test'\n",
    "df_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_author[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text=curr_row['ret']\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_row['user']\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export=pd.DataFrame()\n",
    "df_columns_list =['external_author_id', 'author', 'content', 'region', 'following', 'followers', 'updates', 'post_type',\n",
    " 'account_type', 'retweet', 'account_category']\n",
    "\n",
    "df_export = pd.DataFrame(columns=df_columns_list)\n",
    "\n",
    "for row in df_ats.index:\n",
    "    \n",
    "    curr_row = df_ats.loc[df_ats.index==row]\n",
    "    curr_author = curr_row['user'][row]\n",
    "    external_author_id = curr_author['id']\n",
    "    author =  curr_author['screen_name']\n",
    "    following = curr_author['following']\n",
    "    followers = curr_author['followers_count']\n",
    "    region = curr_author['location']\n",
    "    full_text = curr_row['full_text'][row]\n",
    "    \n",
    "    df_export.loc[row, 'external_author_id'] = external_author_id\n",
    "    df_export.loc[row, 'author'] = author\n",
    "    df_export.loc[row, 'content'] = full_text\n",
    "    df_export.loc[row, 'region'] = region\n",
    "    df_export.loc[row, 'following'] = following\n",
    "    df_export.loc[row, 'followers'] = followers\n",
    "    df_export.loc[row, 'updates'] = np.nan\n",
    "    df_export.loc[row, 'post_type'] = 'control'\n",
    "    df_export.loc[row, 'account_type'] = 'control'\n",
    "    df_export.loc[row, 'retweet'] = curr_row['retweeted'][row]\n",
    "    df_export.loc[row, 'account_category'] = 'control' \n",
    "    df_export.loc[row, 'publish_date'] = curr_row['created_at'][row]\n",
    "    df_export.loc[row, 'language'] = curr_row['lang'][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.to_csv('newly_extracted_control_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
