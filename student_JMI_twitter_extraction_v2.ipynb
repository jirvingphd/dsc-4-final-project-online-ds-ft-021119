{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Million Russian Troll Tweets - Part I\n",
    "- James M Irving, Ph.D.\n",
    "- Mod 4 Project\n",
    "- Flatiron Full Time Data Science Bootcamp - 02/2019 Cohort\n",
    "\n",
    "## GOAL: Using Twitter API to Extract Control Tweets\n",
    "\n",
    "- *IF I can get a control dataset* of non-Troll tweets from same time period with similar hashtags:*\n",
    "    - Use NLP to predict of a tweet is from an authentic user or a Russian troll.\n",
    "- *If no control tweets to compare to*\n",
    "    - Use NLP to predict how many retweets a Troll tweet will get.\n",
    "    - Consider both raw # of retweets, as well as a normalized # of retweets/# of followers.\n",
    "        - The latter would give better indication of language's effect on propagation. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSPECTING TROLL TWEETS FROM KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: bs_ds in /anaconda3/envs/learn-env/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: shap in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.30.2)\n",
      "Requirement already satisfied, skipping upgrade: seaborn in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: ipywidgets in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (7.5.1)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (2.10.3)\n",
      "Requirement already satisfied, skipping upgrade: LIME in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.1.1.36)\n",
      "Requirement already satisfied, skipping upgrade: keras>=2.2.4 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.25.2)\n",
      "Requirement already satisfied, skipping upgrade: jsonpickle in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (1.2)\n",
      "Requirement already satisfied, skipping upgrade: gensim in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: tweepy in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.21.3)\n",
      "Requirement already satisfied, skipping upgrade: catboost in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.17.5)\n",
      "Requirement already satisfied, skipping upgrade: xgboost in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.90)\n",
      "Requirement already satisfied, skipping upgrade: tzlocal in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: cufflinks==0.16 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.16)\n",
      "Requirement already satisfied, skipping upgrade: imgkit in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: openpyxl in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: graphviz in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.13)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: pyperclip in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: pprint in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.1)\n",
      "Requirement already satisfied, skipping upgrade: IPython in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (6.5.0)\n",
      "Requirement already satisfied, skipping upgrade: plotly==3.10.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: fake-useragent in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.1.11)\n",
      "Requirement already satisfied, skipping upgrade: qgrid in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pydotplus in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (4.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (2018.5)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: eli5 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from bs_ds) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>4.25.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from shap->bs_ds) (4.36.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from shap->bs_ds) (0.16.1)\n",
      "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from ipywidgets->bs_ds) (3.5.1)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from ipywidgets->bs_ds) (4.3.2)\n",
      "Requirement already satisfied, skipping upgrade: ipykernel>=4.5.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from ipywidgets->bs_ds) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from ipywidgets->bs_ds) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from jinja2->bs_ds) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from keras>=2.2.4->bs_ds) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from keras>=2.2.4->bs_ds) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from keras>=2.2.4->bs_ds) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from keras>=2.2.4->bs_ds) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from keras>=2.2.4->bs_ds) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from pandas->bs_ds) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: smart_open>=1.2.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from gensim->bs_ds) (1.8.4)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.7.0,>=1.6.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorflow->bs_ds) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tweepy->bs_ds) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: PySocks>=1.5.7 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tweepy->bs_ds) (1.6.8)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.11.1 in /Users/jamesirving/.local/lib/python3.6/site-packages (from tweepy->bs_ds) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-learn->bs_ds) (0.14.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlover>=0.2.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from cufflinks==0.16->bs_ds) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.4.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from cufflinks==0.16->bs_ds) (40.0.0)\n",
      "Requirement already satisfied, skipping upgrade: jdcal in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from openpyxl->bs_ds) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: et-xmlfile in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from openpyxl->bs_ds) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib->bs_ds) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib->bs_ds) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib->bs_ds) (2.4.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: appnope; sys_platform == \"darwin\" in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: backcall in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (4.6.0)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (0.12.1)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.15 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from IPython->bs_ds) (1.0.15)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from plotly==3.10.0->bs_ds) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: notebook>=4.0.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from qgrid->bs_ds) (6.0.1)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>=1.2 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from beautifulsoup4->bs_ds) (1.9.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>16.0.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from eli5->bs_ds) (19.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from eli5->bs_ds) (0.8.5)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image->shap->bs_ds) (6.2.0)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image->shap->bs_ds) (2.2)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image->shap->bs_ds) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image->shap->bs_ds) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: ipython_genutils in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets->bs_ds) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-client in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets->bs_ds) (5.3.3)\n",
      "Requirement already satisfied, skipping upgrade: tornado>=4.2 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets->bs_ds) (6.0.3)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets->bs_ds) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyter_core in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets->bs_ds) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim->bs_ds) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim->bs_ds) (1.9.234)\n",
      "Requirement already satisfied, skipping upgrade: bz2file in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim->bs_ds) (0.98)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow->bs_ds) (0.16.0)\n",
      "Collecting bleach==1.5.0\n",
      "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow->bs_ds) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: html5lib==0.9999999 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow->bs_ds) (0.9999999)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy->bs_ds) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.11.1->tweepy->bs_ds) (2.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.11.1->tweepy->bs_ds) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.11.1->tweepy->bs_ds) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from requests>=2.11.1->tweepy->bs_ds) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->IPython->bs_ds) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: parso>=0.3.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from jedi>=0.10->IPython->bs_ds) (0.3.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->IPython->bs_ds) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: pyzmq>=17 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from notebook>=4.0.0->qgrid->bs_ds) (18.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Send2Trash in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from notebook>=4.0.0->qgrid->bs_ds) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: nbconvert in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from notebook>=4.0.0->qgrid->bs_ds) (5.6.0)\n",
      "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from notebook>=4.0.0->qgrid->bs_ds) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-client in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from notebook>=4.0.0->qgrid->bs_ds) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.234 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim->bs_ds) (1.12.234)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim->bs_ds) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim->bs_ds) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: testpath in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from nbconvert->notebook>=4.0.0->qgrid->bs_ds) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from nbconvert->notebook>=4.0.0->qgrid->bs_ds) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from nbconvert->notebook>=4.0.0->qgrid->bs_ds) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: defusedxml in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from nbconvert->notebook>=4.0.0->qgrid->bs_ds) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from nbconvert->notebook>=4.0.0->qgrid->bs_ds) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /anaconda3/envs/learn-env/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.234->boto3->smart_open>=1.2.1->gensim->bs_ds) (0.15.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: readme-renderer 24.0 has requirement bleach>=2.1.0, but you'll have bleach 1.5.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: bleach\n",
      "  Found existing installation: bleach 3.1.0\n",
      "    Uninstalling bleach-3.1.0:\n",
      "      Successfully uninstalled bleach-3.1.0\n",
      "Successfully installed bleach-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bs_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs_ds  v0.10.0 loaded.  Read the docs: https://bs-ds.readthedocs.io/en/latest/index.html\n",
      "> For convenient loading of standard modules use: `from bs_ds.imports import *`\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Package</th>        <th class=\"col_heading level0 col1\" >Handle</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row0_col0\" class=\"data row0 col0\" >bs_ds</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row0_col1\" class=\"data row0 col1\" >bs</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row0_col2\" class=\"data row0 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row1_col0\" class=\"data row1 col0\" >matplotlib</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row1_col1\" class=\"data row1 col1\" >mpl</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row1_col2\" class=\"data row1 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row2_col0\" class=\"data row2 col0\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row2_col1\" class=\"data row2 col1\" >plt</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row2_col2\" class=\"data row2 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row3_col0\" class=\"data row3 col0\" >numpy</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row3_col1\" class=\"data row3 col1\" >np</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row3_col2\" class=\"data row3 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row4_col0\" class=\"data row4 col0\" >pandas</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row4_col1\" class=\"data row4 col1\" >pd</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row4_col2\" class=\"data row4 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row5_col0\" class=\"data row5 col0\" >seaborn</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row5_col1\" class=\"data row5 col1\" >sns</td>\n",
       "                        <td id=\"T_d3558fd4_ff73_11e9_b8f7_acde48001122row5_col2\" class=\"data row5 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c22f23588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bs_ds as bs\n",
    "from bs_ds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_dir = 'russian-troll-tweets/'\n",
    "# os.listdir('russian-troll-tweets/')\n",
    "filelist = [os.path.join(root_dir,file) for file in os.listdir(root_dir) if file.endswith('.csv')]\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vertically concatenate \n",
    "df = pd.DataFrame()\n",
    "for file in filelist:\n",
    "    df_new = pd.read_csv(file)\n",
    "    df = pd.concat([df,df_new], axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Features:\n",
    "- Kaggle Dataset published by FiveThirtyEight\n",
    "    - https://www.kaggle.com/fivethirtyeight/russian-troll-tweets/downloads/russian-troll-tweets.zip/2\n",
    "<br>    \n",
    "- Data is split into 9 .csv files\n",
    "    - 'IRAhandle_tweets_1.csv' to 9\n",
    "\n",
    "- **Variables:**\n",
    "    - ~~`external_author_id` | An author account ID from Twitter~~\n",
    "    - `author` | The handle sending the tweet\n",
    "    - `content` | The text of the tweet\n",
    "    - `region` | A region classification, as [determined by Social Studio](https://help.salesforce.com/articleView?   id=000199367&type=1)\n",
    "    - `language` | The language of the tweet\n",
    "    - `publish_date` | The date and time the tweet was sent\n",
    "    - ~~`harvested_date` | The date and time the tweet was collected by Social Studio~~\n",
    "    - `following` | The number of accounts the handle was following at the time of the tweet\n",
    "    - `followers` | The number of followers the handle had at the time of the tweet\n",
    "    - `updates` | The number of “update actions” on the account that authored the tweet, including tweets, retweets and likes\n",
    "    - `post_type` | Indicates if the tweet was a retweet or a quote-tweet *[Whats a quote-tweet?]*\n",
    "    - `account_type` | Specific account theme, as coded by Linvill and Warren\n",
    "    - `retweet` | A binary indicator of whether or not the tweet is a retweet [?]\n",
    "    - `account_category` | General account theme, as coded by Linvill and Warren\n",
    "    - `new_june_2018` | A binary indicator of whether the handle was newly listed in June 2018\n",
    "    \n",
    "### **Classification of account_type**\n",
    "Taken from: [rcmediafreedom.eu summary](https://www.rcmediafreedom.eu/Publications/Academic-sources/Troll-Factories-The-Internet-Research-Agency-and-State-Sponsored-Agenda-Building)\n",
    "\n",
    ">- **They identified five categories of IRA-associated Twitter accounts, each with unique patterns of behaviors:**\n",
    "    - **Right Troll**, spreading nativist and right-leaning populist messages. It supported the candidacy and Presidency of Donald Trump and denigrated the Democratic Party. It often sent divisive messages about mainstream and moderate Republicans.\n",
    "    - **Left Troll**, sending socially liberal messages and discussing gender, sexual, religious, and -especially- racial identity. Many tweets seemed intentionally divisive, attacking mainstream Democratic politicians, particularly Hillary Clinton, while supporting Bernie Sanders prior to the election.\n",
    "    - **News Feed**, overwhelmingly presenting themselves as U.S. local news aggregators, linking to legitimate regional news sources and tweeting about issues of local interest.\n",
    "    - **Hashtag Gamer**, dedicated almost exclusively to playing hashtag games.\n",
    "    - **Fearmonger**: spreading a hoax about poisoned turkeys near the 2015 Thanksgiving holiday.\n",
    "\n",
    ">The different types of account were used differently and their efforts were conducted systematically, with different allocation when faced with different political circumstances or shifting goals. E.g.: there was a spike of activity by right and left troll accounts before the publication of John Podesta's emails by WikiLeaks. According to the authors, this activity can be characterised as “industrialized political warfare”.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from Inspection / Pandas_Profiling ProfileReport\n",
    "\n",
    "- **Language to Analyze is in `Content`:**\n",
    "    - Actual tweet contents. \n",
    " \n",
    "- **Classification/Analysis Thoughts:**\n",
    "    - **Variables should be considered in 2 ways:**\n",
    "        - First, the tweet contents. \n",
    "            - Use NLP to engineer features to feed into deep learning.\n",
    "                - Sentiment analysis, named-entity frequency/types, most-similar words. \n",
    "        - Second, the tweet metadata. \n",
    "        \n",
    "### Thoughts on specific features:\n",
    "- `language`\n",
    "    - There are 56 unique languages. \n",
    "    - 2.4 million are English, 670 K are in Russian, etc.\n",
    "\n",
    "### Questions to answer:\n",
    "- [x] Why are so many post_types missing? (55%?)\n",
    "    - Because they were added 'new_june_2018' and were not classified by the original scientists. \n",
    "- [x] How many tweets were written by a russian troll account?\n",
    "    - After removing retweets, there are 1,272,848 original tweets. \n",
    "    \n",
    "### Scrubing to Perform\n",
    "- **Recast Columns:**\n",
    "    - [ ] `publish_date` to datetime. \n",
    "- **Columns to Discard:**\n",
    "    - [ ] `harvested_date` (we care about publish_date, if anything, time-wise)\n",
    "    - [ ] `language`: remove all non-english tweets and drop column\n",
    "    - [ ] `new_june_2018`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Targeted Tweets to Language=English and Retweet=0 Only\n",
    "\n",
    "- Since the goal is to use NLP to detect which tweets came from Russian trolls, we will only analyze the tweets that were originally created by a known Russian troll account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-english rows\n",
    "df = df.loc[df.language=='English']\n",
    "df = df.loc[df.retweet==0]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop harvested_date and new_june_2018\n",
    "cols_to_drop = ['harvested_date','new_june_2018']#: remove all non-english tweets and drop column\n",
    "\n",
    "for col in cols_to_drop:\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Save/Load and Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_or_load = input('Would you like to \"save\" or \"load\" dataframe?\\n(\"save\",\"load\",\"no\"):')\n",
    "\n",
    "# if save_or_load.lower()=='save':\n",
    "#     # Save csv\n",
    "#     df.to_csv('russian_troll_tweets_eng_only_date_pub_index.csv')\n",
    "    \n",
    "# if save_or_load.lower()=='load':\n",
    "#     import bs_ds as bs\n",
    "#     from bs_ds.imports import *\n",
    "#     # Load csva\n",
    "#     df = pd.read_csv('russian_troll_tweets_eng_only_date_pub_index.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i,'\\t',np.random.choice(df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recasting Publish date as datetime column (date_published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recast date_published as datetime and make index\n",
    "df['date_published'] = pd.to_datetime(df['publish_date'])\n",
    "df.set_index('date_published', inplace=True)\n",
    "print('Changed index to datetime \"date_published\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publish_date to datetime\n",
    "# df['date_published'] = pd.to_datetime(df.publish_date)\n",
    "print(f'Tweet dates from {np.min(df.index)}  to  {np.max(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TwitterAPI to Harvest Control Tweets\n",
    "\n",
    "## My Search Strategy\n",
    "\n",
    "- **We need non-troll Tweets to use as a control for the Troll tweets. Ideally, these would be from the same time period covered by the Troll tweets.**\n",
    "\n",
    "    - However, extracting batch historical tweets from the same time period is not an option. This would **require Twitter Enterprise level** Developer Membership (which costs **\\\\$2,000 per month**)\n",
    "    - The free Twitter developer account access allows extracting Tweets from the last 7 days. We will have to work within this limitation for harvesting control tweets.\n",
    "    - Due to the temporal difference, extra care must be put into Tweet search strategy.\n",
    "\n",
    "\n",
    "\n",
    "**Inspect Data to get search parameters:**\n",
    "- [X] Get the date range for the English tweets in the original dataset<br>\n",
    "    - **Tweet date range:**\n",
    "        - **2012-02-06** to **2018-05-30**\n",
    "\n",
    "- [X] Get a list of the hash tags (and their frequencies from the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Data to get search parameters:\n",
    "print(f'Tweet date range:\\n {min(df.index)} to {max(df.index)}')\n",
    "print(f'\\nTotal days: {max(df.index)-min(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Hashtags & @'s to search for\n",
    "\n",
    "- Use regular expressions to extract the hashtags #words and @handles.\n",
    "- Use the top X many tags as search terms for twitter API\n",
    "    - There are _1,678,170 unique hashtags_ and _1,165,744 unique @'s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Make a column containing all hashtags and mentions\n",
    "import re\n",
    "hashtags = re.compile(r'(\\#\\w*)')\n",
    "# df['hashtags'] = df['content'].map(lambda x: hashtags.findall(str(x)))\n",
    "\n",
    "mentions = re.compile(r'(\\@\\w*)')\n",
    "# df['mentions'] = df['content'].map(lambda x: mentions.findall(str(x)))\n",
    "\n",
    "urls = re.compile(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\")\n",
    "# df['links'] = df['content'].map(lambda x: urls.findall(str(x)))\n",
    "\n",
    "# Testing individual re's from above\n",
    "hashtag_list = []\n",
    "hashtag_list = df['content'].map(lambda x: hashtags.findall(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "all_hashtags = []\n",
    "for i in range(len(hashtag_list)):\n",
    "    if len(hashtag_list[i])==0:\n",
    "        continue\n",
    "    elif len(hashtag_list[i])>1:\n",
    "        [all_hashtags.append(x) for x in hashtag_list[i]]\n",
    "\n",
    "    else:\n",
    "        all_hashtags.append(hashtag_list[i])\n",
    "    \n",
    "hashtag_counts = pd.Series(all_hashtags)\n",
    "hashtag_counts.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def get_tags_ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_tags_ats to accept a list of text entries and return all found tags and ats as 2 series/lists\n",
    "def get_tags_ats(text_to_search,exp_tag = r'(#\\w*)',exp_at = r'(@\\w*)', output='series',show_counts=False):\n",
    "    \"\"\"Accepts a list of text entries to search, and a regex for tags, and a regex for @'s.\n",
    "    Joins all entries in the list of text and then re.findsall() for both expressions.\n",
    "    Returns a series of found_tags and a series of found_ats.'\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Create a single long joined-list of strings\n",
    "    text_to_search_combined = ' '.join(text_to_search)\n",
    "        \n",
    "    # print(len(text_to_search_combined), len(text_to_search_list))\n",
    "    found_tags = re.findall(exp_tag, text_to_search_combined)\n",
    "    found_ats = re.findall(exp_at, text_to_search_combined)\n",
    "    \n",
    "    if output.lower() == 'series':\n",
    "        found_tags = pd.Series(found_tags, name='tags')\n",
    "        found_ats = pd.Series(found_ats, name='ats')\n",
    "        \n",
    "        if show_counts==True:\n",
    "            print(f'\\t{found_tags.name}:\\n{tweet_tags.value_counts()} \\n\\n\\t{found_ats.name}:\\n{tweet_ats.value_counts()}')\n",
    "                \n",
    "    if (output.lower() != 'series') & (show_counts==True):\n",
    "        raise Exception('output must be set to \"series\" in order to show_counts')\n",
    "                       \n",
    "    return found_tags, found_ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to get a list of hash tags.\n",
    "text_to_search_list = []\n",
    "\n",
    "for i in range(len(df)):    \n",
    "    tweet_contents =df['content'].iloc[i]\n",
    "    text_to_search_list.append(tweet_contents)\n",
    "\n",
    "text_to_search_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get all tweet tags and @'s from text_to_search_list\n",
    "tweet_tags, tweet_ats = get_tags_ats(text_to_search_list, show_counts=False)\n",
    "\n",
    "print(f\"There were {len(tweet_tags)} unique hashtags and {len(tweet_ats)} unique @'s\\n\")\n",
    "\n",
    "# Create a dataframe with top_tags\n",
    "df_top_tags = pd.DataFrame(tweet_tags.value_counts()[:40])#,'\\n')\n",
    "df_top_tags['% Total'] = (df_top_tags['tags']/len(tweet_tags)*100)\n",
    "\n",
    "# Create a dataframe with top_ats\n",
    "df_top_ats = pd.DataFrame(tweet_ats.value_counts()[:40])\n",
    "df_top_ats['% Total'] = (df_top_ats['ats']/len(tweet_ats)*100)\n",
    "\n",
    "# Display top tags and ats\n",
    "# bs.display_side_by_side(df_top_tags,df_top_ats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Top Tags and Ats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose list of top tags to use in search\n",
    "list_top_30_tags = df_top_tags.index[:30]\n",
    "list_top_30_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose list of top tags to use in search\n",
    "list_top_30_ats = df_top_ats.index[:30]\n",
    "list_top_30_ats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Tweet Search Strategy \n",
    "- **The most common hashtags include some very generic categories** that will not be appropriate control tweets.\n",
    "    - Examples:\n",
    "        - '#news','#sports','#politics','#world','#local','#TopNews','#health','#business','#tech'\n",
    "    - If we used these to extract control Tweets, our model would be biased, since many of these categories contain time-specific topics and would therefore be easy to predict vs. the troll tweets.\n",
    "  \n",
    "- **The most common @'s are much more revealing and helpful in narrowing the focus of the results.**\n",
    "    - Final decision is to use the top 40 mentions from the trolls tweets and extracting present-day Tweets with the same mentions. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Twitter Search API to Extract Control Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Required API key are saved in the Main folder in which this repo is saved. \n",
    "- [x] Check the [Premium account docs for search syntax](https://developer.twitter.com/en/docs/tweets/search/guides/premium-operators.html)\n",
    "- [x] [Check this article for using Tweepy for most efficient twitter api extraction](https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively./)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LINK TO PREMIUM SEARCH API GUIDE**<br>\n",
    "https://developer.twitter.com/en/docs/tweets/search/api-reference/premium-search\n",
    "\n",
    "**Available search operators**\n",
    "- Premium search API supports rules with up to 1,024 characters. The Search Tweets APIs support the premium operators listed below. See our Premium operators guide for more details.\n",
    "\n",
    "- The base URI for the premium search API is https://api.twitter.com/1.1/tweets/search/.\n",
    "\n",
    "**Matching on Tweet contents:**\n",
    "- keyword , \"quoted phrase\" , # , @, url , lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using `tweepy` to access twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def connect_twitter_api, def search_twitter_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Initialzie Tweepy with Authorization Keys    \n",
    "def connect_twitter_api(api_key, api_secret_key):\n",
    "    import tweepy, sys\n",
    "    auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    if (not api):\n",
    "        print(\"Can't authenticate.\")\n",
    "        sys.exit(-1)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def search_twitter_api(api_object, searchQuery, maxTweets, fName, tweetsPerQry=100, max_id=0, sinceId=None):\n",
    "    \"\"\"Take an authenticated tweepy api_object, a search queary, max# of tweets to retreive, a desintation filename.\n",
    "    Uses tweept.api.search for the searchQuery until maxTweets is reached, saved harvest tweets to fName.\"\"\"\n",
    "    import sys, jsonpickle, os\n",
    "    api = api_object\n",
    "    tweetCount = 0\n",
    "    print(f'Downloading max{maxTweets} for {searchQuery}...')\n",
    "    with open(fName, 'a+') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "\n",
    "            try:\n",
    "                if (max_id <=0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, since_id=sinceId, tweet_mode='extended')\n",
    "\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id-1), tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id-1),since_id=sinceId, tweet_mode='extended')\n",
    "\n",
    "                if not new_tweets:\n",
    "                    print('No more tweets found')\n",
    "                    break\n",
    "\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False)+'\\n')\n",
    "\n",
    "                tweetCount+=len(new_tweets)\n",
    "\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "    print (\"Downloaded {0} tweets, Saved to {1}\\n\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Twitter and Harvest Tweets\n",
    "\n",
    "### Making lists of tags and ats to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the # of each @ and each # that i want ot query, then make a query_dict to feed into the cell below\n",
    "query_ats = tuple(zip(df_top_ats.index, df_top_ats['ats']))\n",
    "query_tags = tuple(zip(df_top_tags.index, df_top_tags['tags']))\n",
    "\n",
    "# Calculate how many tweets are represented by the top 30 tags and top 30 @'s \n",
    "sum_top_tweet_tags = df_top_tags['tags'].sum()\n",
    "sum_top_tweet_ats = df_top_ats['ats'].sum()\n",
    "print(f\"Sum of top tags = {sum_top_tweet_tags}\\nSum of top @'s = {sum_top_tweet_ats}\")\n",
    "\n",
    "print(query_ats[:10],'\\n')\n",
    "print(query_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([x[1] for x in query_ats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Data to get search parameters:\n",
    "print(f'Tweet date range:\\n {min(df.index)} to {max(df.index)}')\n",
    "print(f'\\nTotal days: {max(df.index)-min(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to twitter api and searching for lists of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import API keys from text files (so not displayed here and not in repo)\n",
    "with open('../consumer_API_key.txt','r') as f:\n",
    "    api_key =  f.read()\n",
    "with open('../consumer_API_secret_key.txt','r') as f:\n",
    "    api_secret_key  = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually connecting to API and doing test searches. \n",
    "import tweepy, sys\n",
    "auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print(\"Can't authenticate.\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a batch of test results\n",
    "searchQuery='#politics'\n",
    "tweetsPerQry=100\n",
    "\n",
    "new_tweets = api.search(q=searchQuery, count=tweetsPerQry, tweet_mode='extended')\n",
    "type(new_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Display time range of new_tweets so i can define a timetrange to test\n",
    "test_dates = [x.created_at for x in new_tweets]\n",
    "print(f'Range:{min(test_dates)} to {max(test_dates)}')\n",
    "test_dates[0], test_dates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "end_time = datetime(2019,6,2,20,0,0)\n",
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINING A NEW FUNCTION TO EXAMINE THE NEW_TWEETS OUTPUTS\n",
    "def check_tweet_daterange(new_tweets,timerange_begin,timerange_end,verbose=0):\n",
    "    \"\"\"Examines specific information for each tweet in a tweepy searchResults object.\"\"\"\n",
    "    \n",
    "    time_start = timerange_begin\n",
    "    time_end = timerange_end\n",
    "    \n",
    "    # Pull out each tweet's status object. \n",
    "    idx_keep_tweets = []\n",
    "    for i,tweet in enumerate(new_tweets):\n",
    "        if (tweet.created_at > time_start) and (tweet.created_at < time_end):\n",
    "            idx_keep_tweets.append(i)\n",
    "            if verbose>0:\n",
    "                print(f'tweet({i} kept:{tweet.created_at})')\n",
    "    return idx_keep_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining search criteria to limit twitter results to\n",
    "latest_date = max(df.index) # Get latest date from troll tweets\n",
    "earliest_date = min(df.index) # Get the earliest date from troll tweets\n",
    "\n",
    "# Convert pandas timestamps to datetime object for tweet results\n",
    "latest_datetime = latest_date.to_pydatetime()\n",
    "earliest_datetime = earliest_date.to_pydatetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automated Searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = connect_twitter_api(api_key,api_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract tweets for top @'s, while matching the distribution of top @'s\n",
    "final_query_list = query_ats\n",
    "filename = 'tweets_for_top40_ats.txt'\n",
    "\n",
    "for q in final_query_list:\n",
    "    searchQuery = q[0]\n",
    "    maxTweets = q[1]\n",
    "    print(f'Query={searchQuery}, max={maxTweets}')\n",
    "    search_twitter_api(api, searchQuery, maxTweets, fName=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Extracted Tweets from API to match Troll Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats = pd.read_json('tweets_for_top40_ats.txt', lines=True)\n",
    "\n",
    "df_ats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats['entities'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ats.loc[df_ats['full_text'].str.contains('RT')]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re_RT=re.compile('RT \\@\\w*\\:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rt_tweets = df['full_text'][:10]\n",
    "example_rt_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rt_tweets.apply(lambda x: re_RT.sub('',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_list = (['idx','quoted_status'])\n",
    "for k,v in df['quoted_status'].items()[:10]:\n",
    "    print\n",
    "    quote_list.append([k,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweeted_status'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Making New Extracted Tweets Match Russian Troll Tweet Database\n",
    "\n",
    "- Columns to be renamed/reformatted to match troll tweets:\n",
    "    - `created_at` -> `date_published`-> index\n",
    "    - `full_text` -> `content`\n",
    "    - `df['user']`\n",
    "        - `.['followers_count']` -> `'following'`\n",
    "        - `.['followers_count']` -> `'followers'`\n",
    "\n",
    "- Columns missing from original troll tweets (to be removed).\n",
    "    -coordinates, favorited, favorite_count, display_text_range, withheld_in_countries    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats['lang'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats.user[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ats.entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ats['user'][1]['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ats['user'][0]['id'])\n",
    "print(df_ats['user'][0]['screen_name'])\n",
    "print(df_ats['user'][0]['followers_count'])\n",
    "print(df_ats['user'][0]['following'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_row = df_ats.index[2]\n",
    "curr_row = df_ats.loc[df_ats.index==idx_row]\n",
    "curr_author = curr_row['user']\n",
    "curr_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_list =['external_author_id', 'author', 'content', 'region', 'following', 'followers', 'updates', 'post_type',\n",
    " 'account_type', 'retweet', 'account_category']\n",
    "df_export = pd.DataFrame(columns=df_columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.loc[0,'external_author_id']='test'\n",
    "df_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_author[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text=curr_row['ret']\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_row['user']\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export=pd.DataFrame()\n",
    "df_columns_list =['external_author_id', 'author', 'content', 'region', 'following', 'followers', 'updates', 'post_type',\n",
    " 'account_type', 'retweet', 'account_category']\n",
    "\n",
    "df_export = pd.DataFrame(columns=df_columns_list)\n",
    "\n",
    "for row in df_ats.index:\n",
    "    \n",
    "    curr_row = df_ats.loc[df_ats.index==row]\n",
    "    curr_author = curr_row['user'][row]\n",
    "    external_author_id = curr_author['id']\n",
    "    author =  curr_author['screen_name']\n",
    "    following = curr_author['following']\n",
    "    followers = curr_author['followers_count']\n",
    "    region = curr_author['location']\n",
    "    full_text = curr_row['full_text'][row]\n",
    "    \n",
    "    df_export.loc[row, 'external_author_id'] = external_author_id\n",
    "    df_export.loc[row, 'author'] = author\n",
    "    df_export.loc[row, 'content'] = full_text\n",
    "    df_export.loc[row, 'region'] = region\n",
    "    df_export.loc[row, 'following'] = following\n",
    "    df_export.loc[row, 'followers'] = followers\n",
    "    df_export.loc[row, 'updates'] = np.nan\n",
    "    df_export.loc[row, 'post_type'] = 'control'\n",
    "    df_export.loc[row, 'account_type'] = 'control'\n",
    "    df_export.loc[row, 'retweet'] = curr_row['retweeted'][row]\n",
    "    df_export.loc[row, 'account_category'] = 'control' \n",
    "    df_export.loc[row, 'publish_date'] = curr_row['created_at'][row]\n",
    "    df_export.loc[row, 'language'] = curr_row['lang'][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.to_csv('newly_extracted_control_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
