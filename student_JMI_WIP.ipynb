{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Million Russian Troll Tweets\n",
    "- James M Irving, Ph.D.\n",
    "- Mod 4 Project\n",
    "- Flatiron Full Time Data Science Bootcamp - 02/2019 Cohort\n",
    "\n",
    "## GOAL: \n",
    "\n",
    "- *IF I can get a control dataset* of non-Troll tweets from same time period with similar hashtags:*\n",
    "    - Use NLP to predict of a tweet is from an authentic user or a Russian troll.\n",
    "- *If no control tweets to compare to*\n",
    "    - Use NLP to predict how many retweets a Troll tweet will get.\n",
    "    - Consider both raw # of retweets, as well as a normalized # of retweets/# of followers.\n",
    "        - The latter would give better indication of language's effect on propagation. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Features:\n",
    "- Kaggle Dataset published by FiveThirtyEight\n",
    "    - https://www.kaggle.com/fivethirtyeight/russian-troll-tweets/downloads/russian-troll-tweets.zip/2\n",
    "<br>    \n",
    "- Data is split into 9 .csv files\n",
    "    - 'IRAhandle_tweets_1.csv' to 9\n",
    "\n",
    "- **Variables:**\n",
    "    - ~~`external_author_id` | An author account ID from Twitter~~\n",
    "    - `author` | The handle sending the tweet\n",
    "    - `content` | The text of the tweet\n",
    "    - `region` | A region classification, as [determined by Social Studio](https://help.salesforce.com/articleView?   id=000199367&type=1)\n",
    "    - `language` | The language of the tweet\n",
    "    - `publish_date` | The date and time the tweet was sent\n",
    "    - ~~`harvested_date` | The date and time the tweet was collected by Social Studio~~\n",
    "    - `following` | The number of accounts the handle was following at the time of the tweet\n",
    "    - `followers` | The number of followers the handle had at the time of the tweet\n",
    "    - `updates` | The number of “update actions” on the account that authored the tweet, including tweets, retweets and likes\n",
    "    - `post_type` | Indicates if the tweet was a retweet or a quote-tweet *[Whats a quote-tweet?]*\n",
    "    - `account_type` | Specific account theme, as coded by Linvill and Warren\n",
    "    - `retweet` | A binary indicator of whether or not the tweet is a retweet [?]\n",
    "    - `account_category` | General account theme, as coded by Linvill and Warren\n",
    "    - `new_june_2018` | A binary indicator of whether the handle was newly listed in June 2018\n",
    "    \n",
    "### **Classification of account_type**\n",
    "Taken from: [rcmediafreedom.eu summary](https://www.rcmediafreedom.eu/Publications/Academic-sources/Troll-Factories-The-Internet-Research-Agency-and-State-Sponsored-Agenda-Building)\n",
    "\n",
    ">- **They identified five categories of IRA-associated Twitter accounts, each with unique patterns of behaviors:**\n",
    "    - **Right Troll**, spreading nativist and right-leaning populist messages. It supported the candidacy and Presidency of Donald Trump and denigrated the Democratic Party. It often sent divisive messages about mainstream and moderate Republicans.\n",
    "    - **Left Troll**, sending socially liberal messages and discussing gender, sexual, religious, and -especially- racial identity. Many tweets seemed intentionally divisive, attacking mainstream Democratic politicians, particularly Hillary Clinton, while supporting Bernie Sanders prior to the election.\n",
    "    - **News Feed**, overwhelmingly presenting themselves as U.S. local news aggregators, linking to legitimate regional news sources and tweeting about issues of local interest.\n",
    "    - **Hashtag Gamer**, dedicated almost exclusively to playing hashtag games.\n",
    "    - **Fearmonger**: spreading a hoax about poisoned turkeys near the 2015 Thanksgiving holiday.\n",
    "\n",
    ">The different types of account were used differently and their efforts were conducted systematically, with different allocation when faced with different political circumstances or shifting goals. E.g.: there was a spike of activity by right and left troll accounts before the publication of John Podesta's emails by WikiLeaks. According to the authors, this activity can be characterised as “industrialized political warfare”.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs_ds as bs\n",
    "from bs_ds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = 'russian-troll-tweets/'\n",
    "# os.listdir('russian-troll-tweets/')\n",
    "filelist = [os.path.join(root_dir,file) for file in os.listdir(root_dir) if file.endswith('.csv')]\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing dataset\n",
    "df = pd.read_csv(filelist[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertically concatenate \n",
    "for file in filelist:\n",
    "    df_new = pd.read_csv(file)\n",
    "    df = pd.concat([df,df_new], axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUBBING/EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from Inspection / Pandas_Profiling ProfileReport\n",
    "\n",
    "- **Language to Analyze is in `Content`:**\n",
    "    - Actual tweet contents. \n",
    " \n",
    "- **Classification/Analysis Thoughts:**\n",
    "    - **Variables should be considered in 2 ways:**\n",
    "        - First, the tweet contents. \n",
    "            - Use NLP to engineer features to feed into deep learning.\n",
    "                - Sentiment analysis, named-entity frequency/types, most-similar words. \n",
    "        - Second, the tweet metadata. \n",
    "        \n",
    "### Thoughts on specific features:\n",
    "- `language`\n",
    "    - There are 56 unique languages. \n",
    "    - 2.4 million are English, 670 K are in Russian, etc.\n",
    "    - Note: for metadata, analyzing if an account posts in more than 1 language may be a good predictor. \n",
    "- `followers`/`following`\n",
    "    - **following** could be informative if goal is to predict if its a troll tweet.\n",
    "    - **followers** should be used (with retweets) if predicting retweets based on content. \n",
    "\n",
    "- **Questions:**\n",
    "    - [ ] Why are so many post_types missing? (55%?)\n",
    "    \n",
    "### Scrubing to Perform\n",
    "- **Recast Columns:**\n",
    "    - [ ] `publish_date` to datetime. \n",
    "- **Columns to Discard:**\n",
    "    - [ ] `external_author_id` ( we have author handle)\n",
    "    - [ ] `harvested_date` (we care about publish_date, if anything, time-wise)\n",
    "    - [ ] `language`: remove all non-english tweets and drop column\n",
    "    - [ ] `new_june_2018`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-english rows\n",
    "df = df.loc[df.language=='English']\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['external_author_id','harvested_date','new_june_2018']#: remove all non-english tweets and drop column\n",
    "\n",
    "for col in cols_to_drop:\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recasting Publish date as datetime column (date_published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publish_date to datetime\n",
    "df['date_published'] = pd.to_datetime(df.publish_date)\n",
    "print(np.max(df.date_published), np.min(df.date_published))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date_published',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Save/Load and Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv\n",
    "# df.to_csv('russian_troll_tweets_eng_only_date_pub_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs.check_unique(df,['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs_ds as bs\n",
    "from bs_ds.imports import *\n",
    "# Load csva\n",
    "df = pd.read_csv('russian_troll_tweets_eng_only_date_pub_index.csv')\n",
    "\n",
    "# Recast date_published as datetime and make index\n",
    "df.date_published = pd.to_datetime(df['date_published'])\n",
    "df.set_index('date_published', inplace=True)\n",
    "print('Changed index to datetime \"date_published\".')\n",
    "\n",
    "# Drop un-needed columns\n",
    "cols_to_drop = ['publish_date','language']\n",
    "for col in cols_to_drop:\n",
    "    \n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    print(f'Dropped {col}.')\n",
    "    \n",
    "    \n",
    "# Recast categorical columns\n",
    "cols_to_cats = ['region','post_type','account_type','account_category']\n",
    "for col in cols_to_cats:\n",
    "    \n",
    "    df[col] = df[col].astype('category')\n",
    "    print(f'Converted {col} to category.')\n",
    "\n",
    "\n",
    "# Drop problematic nan in 'contet'\n",
    "df.dropna(subset=['content'],inplace=True) # Dropping the 1 null value \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs.big_pandas()\n",
    "# pd.set_option('display.max_info_columns',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts on My Search Strategy\n",
    "\n",
    "**My Twitter API Link:**<br>\n",
    "https://api.twitter.com/1.1/tweets/search/fullarchive/search.json\n",
    "\n",
    "\n",
    "**Inspect Data to get search parameters:**\n",
    "- [X] Get the date range for the English tweets in the original dataset<br>\n",
    "    - **Tweet date range:**\n",
    "        - **2012-02-06** to **2018-05-30**\n",
    "\n",
    "- [X] Get a list of the hash tags (and their frequencies from the dataframe\n",
    "\n",
    "**Determine most feasible and balanced well of extracting control tweets**\n",
    "- [ ] How many of each tag / @'s should I try to exctract?\n",
    "- [ ] what are the limitations of the API that will be a road block to getting as many tweets as desired?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Data to get search parameters:\n",
    "print(f'Tweet date range:\\n {min(df.index)} to {max(df.index)}')\n",
    "print(f'\\nTotal days: {max(df.index)-min(df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Hashtags & @'s\n",
    "\n",
    "- Use regular expressions to extract the hashtags #words and @handles.\n",
    "- Use the top X many tags as search terms for twitter API\n",
    "    - There are _1,678,170 unique hashtags_ and _1,165,744 unique @'s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_tags_ats to accept a list of text entries and return all found tags and ats as 2 series/lists\n",
    "def get_tags_ats(text_to_search,exp_tag = r'(#\\w*)',exp_at = r'(@\\w*)', output='series',show_counts=False):\n",
    "    \"\"\"Accepts a list of text entries to search, and a regex for tags, and a regex for @'s.\n",
    "    Joins all entries in the list of text and then re.findsall() for both expressions.\n",
    "    Returns a series of found_tags and a series of found_ats.'\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Create a single long joined-list of strings\n",
    "    text_to_search_combined = ' '.join(text_to_search)\n",
    "        \n",
    "    # print(len(text_to_search_combined), len(text_to_search_list))\n",
    "    found_tags = re.findall(exp_tag, text_to_search_combined)\n",
    "    found_ats = re.findall(exp_at, text_to_search_combined)\n",
    "    \n",
    "    if output.lower() == 'series':\n",
    "        found_tags = pd.Series(found_tags, name='tags')\n",
    "        found_ats = pd.Series(found_ats, name='ats')\n",
    "        \n",
    "        if show_counts==True:\n",
    "            print(f'\\t{found_tags.name}:\\n{tweet_tags.value_counts()} \\n\\n\\t{found_ats.name}:\\n{tweet_ats.value_counts()}')\n",
    "                \n",
    "    if (output.lower() != 'series') & (show_counts==True):\n",
    "        raise Exception('output must be set to \"series\" in order to show_counts')\n",
    "                       \n",
    "    return found_tags, found_ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to get a list of hash tags.\n",
    "text_to_search_list = []\n",
    "\n",
    "for i in range(len(df)):    \n",
    "    tweet_contents =df['content'].iloc[i]\n",
    "    text_to_search_list.append(tweet_contents)\n",
    "\n",
    "text_to_search_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get all tweet tags and @'s from text_to_search_list\n",
    "tweet_tags, tweet_ats = get_tags_ats(text_to_search_list, show_counts=False)\n",
    "\n",
    "print(f\"There were {len(tweet_tags)} unique hashtags and {len(tweet_ats)} unique @'s\\n\")\n",
    "\n",
    "# Create a dataframe with top_tags\n",
    "df_top_tags = pd.DataFrame(tweet_tags.value_counts()[:40])#,'\\n')\n",
    "df_top_tags['% Total'] = (df_top_tags['tags']/len(tweet_tags)*100)\n",
    "\n",
    "# Create a dataframe with top_ats\n",
    "df_top_ats = pd.DataFrame(tweet_ats.value_counts()[:40])\n",
    "df_top_ats['% Total'] = (df_top_ats['ats']/len(tweet_ats)*100)\n",
    "\n",
    "# Display top tags and ats\n",
    "# bs.display_side_by_side(df_top_tags,df_top_ats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Top Tags and Ats:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most common tags include some very generic categories that may not be helpful in extracting control tweets.\n",
    "    - ~~Exclude: '#news','#sports','#politics','#world','#local','#TopNews','#health','#business','#tech',~~\n",
    "    - On second thought, this is entirely appropriate, since these tags would be what appears in the wild.\n",
    "    - Additionally, using a larger number of them (like 30, starts to provide more targeted hashtags.<br><br>\n",
    "  \n",
    "- **The most common @'s are much more revealing and helpful in narrowing the focus of the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose list of top tags to use in search\n",
    "list_top_30_tags = df_top_tags.index[:30]\n",
    "list_top_30_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose list of top tags to use in search\n",
    "list_top_30_ats = df_top_ats.index[:30]\n",
    "list_top_30_ats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Twitter Search API to Extract Control Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Required API key are saved in the Main folder in which this repo is saved. \n",
    "- [x] Check the [Premium account docs for search syntax](https://developer.twitter.com/en/docs/tweets/search/guides/premium-operators.html)\n",
    "- [x] [Check this article for using Tweepy for most efficient twitter api extraction](https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively./)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LINK TO PREMIUM SEARCH API GUIDE**<br>\n",
    "https://developer.twitter.com/en/docs/tweets/search/api-reference/premium-search\n",
    "\n",
    "### Available operators\n",
    "- Premium search API supports rules with up to 1,024 characters. The Search Tweets APIs support the premium operators listed below. See our Premium operators guide for more details.\n",
    "\n",
    "- The base URI for the premium search API is https://api.twitter.com/1.1/tweets/search/.\n",
    "\n",
    "**Matching on Tweet contents:**\n",
    "- keyword\n",
    "- \"quoted phrase\"\n",
    "- #\n",
    "- @\n",
    "- url:\n",
    "- lang:\n",
    "\n",
    "\n",
    "**API Post Search Methods**\n",
    "- POST /search/:product/:label\t\n",
    "    - Retrieve Tweets matching the specified query.\n",
    "- POST /search/:product/:label/counts\t\n",
    "    - Retrieve the number of Tweets matching the specified query.\n",
    "- where:\n",
    "    - `:product` indicates the search endpoint you are making requests to, either 30day or fullarchive.\n",
    "    - `:label` is the (case-sensitive) label associated with your search developer environment, as displayed at https://developer.twitter.com/en/account/environments.\n",
    "- For example, if using the 30-day endpoint and your dev environment has a label of 'dev' (short for development), the search URLs would be:\n",
    "    - Data endpoint providing Tweets: https://api.twitter.com/1.1/tweets/search/30day/dev.json\n",
    "    - Counts endpoint providing counts of Tweets: https://api.twitter.com/1.1/tweets/search/30day/dev/counts.json\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tweepy to access twitter API\n",
    "\n",
    "- [Helpful tutorial on _most efficient_ way to access twitter API](https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively./)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import API keys from text files (so not displayed here and not in repo)\n",
    "with open('../consumer_API_key.txt','r') as f:\n",
    "    api_key =  f.read()\n",
    "with open('../consumer_API_secret_key.txt','r') as f:\n",
    "    api_secret_key  = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzie Tweepy with Authorization Keys    \n",
    "def connect_twitter_api(api_key, api_secret_key):\n",
    "    import tweepy, sys\n",
    "    auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    if (not api):\n",
    "        print(\"Can't authenticate.\")\n",
    "        sys.exit(-1)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = connect_twitter_api(api_key,api_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_ats.ats[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the # of each @ and each # that i want ot query, then make a query_dict to feed into the cell below\n",
    "query_ats = tuple(zip(df_top_ats.index, df_top_ats['ats']))\n",
    "auery_tags = tuple(zip(df_top_tags.index, df_top_tags['tags']))\n",
    "\n",
    "# Calculate how many tweets are represented by the top 30 tags and top 30 @'s \n",
    "sum_top_tweet_tags = df_top_tags['tags'].sum()\n",
    "sum_top_tweet_ats = df_top_ats['ats'].sum()\n",
    "print(f\"Sum of top tags = {sum_top_tweet_tags}\\nSum of top @'s = {sum_top_tweet_ats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_twitter_api(api_object, searchQuery, maxTweets, fName, tweetsPerQry=100, max_id=0, sinceId=None):\n",
    "    \"\"\"Take an authenticated tweepy api_object, a search queary, max# of tweets to retreive, a desintation filename.\n",
    "    Uses tweept.api.search for the searchQuery until maxTweets is reached, saved harvest tweets to fName.\"\"\"\n",
    "    import sys, jsonpickle, os\n",
    "\n",
    "    tweetCount = 0\n",
    "    print(f'Downloading max{maxTweets} for {searchQuery}...')\n",
    "    with open(fName, 'a+') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "\n",
    "            try:\n",
    "                if (max_id <=0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, since_id=sinceId, tweet_mode='extended')\n",
    "\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id-1), tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, max_id=str(max_id-1),since_id=sinceId, tweet_mode='extended')\n",
    "\n",
    "                if not new_tweets:\n",
    "                    print('No more tweets found')\n",
    "                    break\n",
    "\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False)+'\\n')\n",
    "\n",
    "                tweetCount+=len(new_tweets)\n",
    "\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "    print (\"Downloaded {0} tweets, Saved to {1}\\n\".format(tweetCount, fName))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract tweets for top @'s, while matching the distribution of top @'s\n",
    "\n",
    "filename = 'top_tweets_by_at.txt'\n",
    "\n",
    "for q in query_ats[:3]:\n",
    "    searchQuery = q[0]\n",
    "    maxTweets = q[1]\n",
    "    print(f'Query={searchQuery}, max={maxTweets}')\n",
    "    search_twitter_api(api, searchQuery, maxTweets, fName=filename)\n",
    "#     time.sleep(1)\n",
    "    \n",
    "# toc(t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_json('top_tweets_by_at_06022019.txt', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['date_published'] = pd.to_datetime(df_tweets['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['date_published'].min(), df_tweets['date_published'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
